{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installs \n\n!pip install mlflow==1.30.1 dagshub==0.2.12 progressbar2==4.2.0 GPUtil==1.4.0 albumentations==1.3.0\n\n# Imports\n\nimport albumentations as A\nimport cv2\nfrom dataclasses import dataclass, asdict\nimport dagshub\nfrom GPUtil import showUtilization as gpu_usage\nimport matplotlib.pyplot as plt\nimport mlflow\nfrom numba import cuda\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport progressbar\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torchvision import models, transforms, utils\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch import flatten\nfrom torchmetrics import MeanSquaredError\n\n# General utilities \n\ndef set_seeds():\n    torch.manual_seed(0)\n    np.random.seed(0)\n    torch.cuda.manual_seed(0)\n    torch.cuda.manual_seed_all(0)\n    torch.backends.cudnn.deterministic=True\n    \ndef free_gpu_cache():\n    print(\"Initial GPU usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU usage after emptying the cache\")\n    gpu_usage()","metadata":{"execution":{"iopub.status.busy":"2023-05-07T19:06:38.338423Z","iopub.execute_input":"2023-05-07T19:06:38.338761Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting mlflow==1.30.1\n  Downloading mlflow-1.30.1-py3-none-any.whl (17.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting dagshub==0.2.12\n  Downloading dagshub-0.2.12-py3-none-any.whl (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: progressbar2==4.2.0 in /opt/conda/lib/python3.10/site-packages (4.2.0)\nCollecting GPUtil==1.4.0\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: albumentations==1.3.0 in /opt/conda/lib/python3.10/site-packages (1.3.0)\nCollecting databricks-cli<1,>=0.8.7\n  Downloading databricks-cli-0.17.6.tar.gz (82 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (8.1.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (0.4.3)\nRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (2.28.2)\nRequirement already satisfied: docker<7,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (6.0.1)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (3.1.31)\nRequirement already satisfied: packaging<22 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (21.3)\nRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (6.0)\nRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (1.23.5)\nRequirement already satisfied: alembic<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (1.10.3)\nRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (1.9.3)\nRequirement already satisfied: cloudpickle<3 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (2.2.1)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (3.20.3)\nCollecting sqlalchemy<2,>=1.4.0\n  Downloading SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting querystring-parser<2\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nCollecting prometheus-flask-exporter<1\n  Downloading prometheus_flask_exporter-0.22.4-py3-none-any.whl (18 kB)\nCollecting pytz<2023\n  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (1.5.3)\nCollecting importlib-metadata!=4.7.0,<6,>=3.7.0\n  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\nRequirement already satisfied: Flask<3 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (2.2.3)\nCollecting gunicorn<21\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.10/site-packages (from mlflow==1.30.1) (0.4)\nCollecting fusepy>=3\n  Downloading fusepy-3.0.1.tar.gz (11 kB)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set up\n\nset_seeds()\n\nDIR_PATH = \"/kaggle/input/facial-keypoints-detection/\"\ntraining_data = pd.read_csv(f\"{DIR_PATH}training.zip\")\ntest_data = pd.read_csv(f\"{DIR_PATH}test.zip\")\nid_lookup_table = pd.read_csv(f\"{DIR_PATH}IdLookupTable.csv\")\n\n@dataclass  \nclass ModelParams:\n    BATCH_SIZE: int = 64\n    VALID_SIZE: float = 0.2\n    N_EPOCHS: int = 100\n    IMG_SIZE: int = 96\n    OUTPUT_SIZE: int = 30 \n    S_OUTPUT_SIZE: int = 8\n    L_OUTPUT_SIZE: int = 22\n    LEARNING_RATE: float = 0.01\n\ndagshub.init(\"facial_reg_model\", \"caddis90\", mlflow=True)\nmlflow.set_tracking_uri('https://dagshub.com/caddis90/facial_reg_model.mlflow')\nmlflow.set_experiment(experiment_name=\"cnn\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using {device}\")\n\nl_dataset_cols = [\n    'left_eye_inner_corner_x','left_eye_inner_corner_y', 'left_eye_outer_corner_x',\n    'left_eye_outer_corner_y', 'right_eye_inner_corner_x','right_eye_inner_corner_y',\n    'right_eye_outer_corner_x','right_eye_outer_corner_y', 'left_eyebrow_inner_end_x',\n    'left_eyebrow_inner_end_y', 'left_eyebrow_outer_end_x','left_eyebrow_outer_end_y',\n    'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y', 'right_eyebrow_outer_end_x',\n    'right_eyebrow_outer_end_y', 'mouth_left_corner_x', 'mouth_left_corner_y',\n    'mouth_right_corner_x', 'mouth_right_corner_y', 'mouth_center_top_lip_x',\n    'mouth_center_top_lip_y', 'Image']\n\ns_dataset_cols = [\n    'left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x',\n    'right_eye_center_y','nose_tip_x', 'nose_tip_y',\n    'mouth_center_bottom_lip_x','mouth_center_bottom_lip_y','Image']\n\nl_dataset = training_data[l_dataset_cols].dropna()\ns_dataset = training_data[s_dataset_cols].dropna()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T21:16:36.699299Z","iopub.execute_input":"2023-05-06T21:16:36.699985Z","iopub.status.idle":"2023-05-06T21:16:40.711902Z","shell.execute_reply.started":"2023-05-06T21:16:36.699951Z","shell.execute_reply":"2023-05-06T21:16:40.710961Z"},"trusted":true},"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/plain":"Repository initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Using cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data \n\nclass FacialKeypointsDataset(Dataset):\n    \n    def __init__(self, dataset, train=True, transform=None):\n        self.dataset = dataset\n        self.train = train\n        self.transform = transform\n\n    def get_image(self, idx):\n        image = np.fromstring(self.dataset.iloc[idx, -1], sep=' ', dtype = np.uint8)\n        image = image.astype(np.float32)\n        image = image.reshape(ModelParams.IMG_SIZE, ModelParams.IMG_SIZE, 1) \n        \n        return image\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):        \n        if self.train:\n            keypoints = self.dataset.iloc[idx, :-1].values.astype(np.float32)\n            total_keypoints = int(len(keypoints)/2)\n            keypoints = keypoints.reshape([total_keypoints, 2])\n        else:\n            keypoints = None\n        \n        if self.transform:\n            data_cols = self.dataset.columns.tolist()\n            sample = self.transform(image=self.get_image(idx), keypoints=keypoints, class_labels=data_cols[0:-1])\n            sample[\"keypoints\"] = torch.tensor(list(sum(sample[\"keypoints\"], ()))).float()\n        else:\n            sample = {\"image\": self.get_image(idx)}\n            \n        sample[\"image\"] = torch.from_numpy(sample[\"image\"].transpose(2, 0, 1)).float()\n        sample[\"image\"] = sample[\"image\"] / 255\n        \n        return sample\n        \n        \ndef prepare_dataloaders(dataset, valid_size, batch_size):\n    dataset_len = len(dataset)\n    dataset_indices = list(range(dataset_len))\n    np.random.shuffle(dataset_indices)\n    split = int(np.floor(valid_size * dataset_len))\n    train_idx, valid_idx = dataset_indices[split:], dataset_indices[:split]\n    \n    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_idx))\n    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(valid_idx))\n    \n    return train_loader, valid_loader\n\n\n# Utilities\n\ndef show_image(image, training_key_points, test_key_points=[]):  \n    plt.imshow(image, cmap=\"gray\")\n\n    training_key_points = training_key_points.reshape([15, 2])\n    plt.plot(training_key_points[:,0], training_key_points[:,1], 'gx')\n    \n    if len(test_key_points) > 0:        \n        test_key_points = test_key_points.reshape([15, 2])\n        plt.plot(test_key_points[:,0], test_key_points[:,1], 'rx')\n\n\ndef train(train_loader, valid_loader, model, optimizer, scheduler):\n    with mlflow.start_run():\n        mlflow.log_params(asdict(ModelParams()))\n        for epoch in progressbar.progressbar(range(ModelParams.N_EPOCHS)):\n            epoch_train_loss, epoch_valid_loss = 0.0, 0.0\n\n            model.train() \n            for i, batch in enumerate(train_loader):\n                optimizer.zero_grad()\n                output = model(batch['image'].to(device))\n                loss = criterion(output, batch['keypoints'].to(device))\n                loss.backward()\n                optimizer.step()\n                epoch_train_loss += loss.item()*batch['image'].size(0)\n                batch_train_rmse = rmse(output.cpu(), batch['keypoints'].cpu())\n\n            epoch_train_rmse = rmse.compute()\n            rmse.reset()\n            scheduler.step()\n\n            with torch.no_grad():\n                model.eval() \n                for i, batch in enumerate(valid_loader):\n                    output = model(batch['image'].to(device))\n                    loss = criterion(output, batch['keypoints'].to(device))\n                    epoch_valid_loss += loss.item()*batch['image'].size(0)\n                    batch_valid_rmse = rmse(output.cpu(), batch['keypoints'].cpu())\n\n                epoch_valid_rmse = rmse.compute()\n                rmse.reset()\n\n                epoch_train_loss = np.sqrt(epoch_train_loss/len(train_loader.sampler.indices))\n                epoch_valid_loss = np.sqrt(epoch_valid_loss/len(valid_loader.sampler.indices))\n\n                mlflow.log_metric(\"train_loss\", epoch_train_loss, step=epoch)\n                mlflow.log_metric(\"valid_loss\", epoch_valid_loss, step=epoch)\n                mlflow.log_metric(\"train_rmse\", epoch_train_rmse, step=epoch)\n                mlflow.log_metric(\"valid_rmse\", epoch_valid_rmse, step=epoch)\n            \ndef predict(model, test_loader):    \n    model.eval()\n    with torch.no_grad():\n        for i, batch in progressbar.progressbar(enumerate(test_loader)):\n            output = model(batch['image'].to(device)).cpu().numpy()\n            output = np.clip(output, a_min=0, a_max=ModelParams.IMG_SIZE)\n            if i == 0:\n                test_predictions = output\n            else:\n                test_predictions = np.vstack((test_predictions, output))\n        \n    return test_predictions\n\ndef create_submission(predictions, prediction_features, id_lookup_table=id_lookup_table):\n    features = list(id_lookup_table['FeatureName'])\n    img_ids = list(id_lookup_table['ImageId']-1) \n\n    prediction_indices = [prediction_features.index(feature) for feature in features]\n\n    submission = pd.DataFrame({\n        \"RowId\": list(id_lookup_table['RowId']),\n        \"Location\": [predictions[x][y] for x, y in zip(img_ids, prediction_indices)]\n    })\n    submission.to_csv(\"submission.csv\",index = False)\n    print(\"Submission successful!\")\n       \n# Model\n\nl_resnet50 = models.resnet50(num_classes = ModelParams.L_OUTPUT_SIZE)\nl_resnet50.inplanes = ModelParams.IMG_SIZE\nl_resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\ns_resnet50 = models.resnet50(num_classes = ModelParams.S_OUTPUT_SIZE)\ns_resnet50.inplanes = ModelParams.IMG_SIZE\ns_resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\ntransformations = A.Compose([\n    A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.8),\n    A.Affine(shear=15, scale=1.0, p=0.2),\n    A.RandomBrightnessContrast(contrast_limit=0.5, brightness_limit=0.5, p=0.2),\n    A.OneOf([\n            A.GaussNoise(p=0.8),\n            A.RandomGamma(p=0.8),\n            A.Blur(p=0.8),\n        ], p=1.0),     \n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.2, border_mode=cv2.BORDER_CONSTANT),\n    ],\n    keypoint_params = A.KeypointParams(format = 'xy', remove_invisible=False)\n)\n\ns_trainset = FacialKeypointsDataset(s_dataset, transform=transformations)\nl_trainset = FacialKeypointsDataset(l_dataset, transform=transformations)\ntestset = FacialKeypointsDataset(test_data, train=False)\n\ns_train_loader, s_valid_loader = prepare_dataloaders(\n        s_trainset, \n        valid_size=ModelParams.VALID_SIZE, \n        batch_size=ModelParams.BATCH_SIZE\n)\nl_train_loader, l_valid_loader = prepare_dataloaders(\n        l_trainset, \n        valid_size=ModelParams.VALID_SIZE, \n        batch_size=ModelParams.BATCH_SIZE\n)\ntest_loader = DataLoader(testset, batch_size=ModelParams.BATCH_SIZE)\n\nl_model = l_resnet50\nl_model = l_model.to(device)\ns_model = s_resnet50\ns_model = s_model.to(device)\n\ncriterion = nn.MSELoss().to(device)\nrmse = MeanSquaredError(squared=False).to(device)\n\nl_optimizer = optim.Adam(l_model.parameters(), lr=ModelParams.LEARNING_RATE)\nl_scheduler = ReduceLROnPlateau(\n    optimizer=l_scheduler, \n    mode=\"min\", \n    factor=0.5,\n    patience=5,\n    min_lr=1e-15\n)\n\ns_optimizer = optim.Adam(s_model.parameters(), lr=ModelParams.LEARNING_RATE)\ns_scheduler = ReduceLROnPlateau(\n    optimizer=s_optimizer, \n    mode=\"min\", \n    factor=0.5,\n    patience=5,\n    min_lr=1e-15\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T21:16:52.278805Z","iopub.execute_input":"2023-05-06T21:16:52.279162Z","iopub.status.idle":"2023-05-06T21:16:53.169462Z","shell.execute_reply.started":"2023-05-06T21:16:52.279134Z","shell.execute_reply":"2023-05-06T21:16:53.168516Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"train(\n    train_loader=l_train_loader, \n    valid_loader=l_valid_loader, \n    model=l_model, \n    optimizer=l_optimizer, \n    scheduler=l_scheduler\n)\n\ntrain(\n    train_loader=s_train_loader, \n    valid_loader=s_valid_loader, \n    model=s_model, \n    optimizer=s_optimizer, \n    scheduler=s_scheduler\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T21:16:58.173818Z","iopub.execute_input":"2023-05-06T21:16:58.174165Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"100% (100 of 100) |######################| Elapsed Time: 0:14:09 Time:  0:14:09\n  5% (5 of 100) |#                       | Elapsed Time: 0:02:06 ETA:   0:39:25","output_type":"stream"}]},{"cell_type":"code","source":"l_predictions = predict(model=l_model, test_loader=test_loader)\ns_predictions = predict(model=s_model, test_loader=test_loader)\npredictions = np.hstack((l_predictions, s_predictions))\nprediction_features = l_dataset_cols[:-1] + s_dataset_cols[:-1]\ncreate_submission(predictions=predictions, prediction_features=prediction_features)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:05:38.082937Z","iopub.execute_input":"2023-05-06T19:05:38.083292Z","iopub.status.idle":"2023-05-06T19:05:44.216586Z","shell.execute_reply.started":"2023-05-06T19:05:38.083263Z","shell.execute_reply":"2023-05-06T19:05:44.215479Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"| |                             #                    | 13 Elapsed Time: 0:00:02\n| |                           #                      | 13 Elapsed Time: 0:00:02\n","output_type":"stream"},{"name":"stdout","text":"Submission successful!\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    test_key_points = model(plot_sample['image'].unsqueeze(0).to(device)).cpu().numpy()","metadata":{},"execution_count":null,"outputs":[]}]}