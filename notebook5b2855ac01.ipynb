{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installs \n\n# !pip install mlflow==1.30.1 dagshub==0.2.12 progressbar2==4.2.0 GPUtil==1.4.0 albumentations==1.3.0\n\n# Imports\n\nimport albumentations as A\nimport cv2\nfrom dataclasses import dataclass, asdict\nimport dagshub\nfrom GPUtil import showUtilization as gpu_usage\nimport matplotlib.pyplot as plt\nimport mlflow\nfrom numba import cuda\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport progressbar\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\nfrom torchvision import models, transforms, utils\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch import flatten\nfrom torchmetrics import MeanSquaredError\n\n# General utilities \n\ndef set_seeds():\n    torch.manual_seed(0)\n    np.random.seed(0)\n    torch.cuda.manual_seed(0)\n    torch.cuda.manual_seed_all(0)\n    torch.backends.cudnn.deterministic=True\n    \ndef free_gpu_cache():\n    print(\"Initial GPU usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU usage after emptying the cache\")\n    gpu_usage()","metadata":{"execution":{"iopub.status.busy":"2023-05-07T19:08:22.225112Z","iopub.execute_input":"2023-05-07T19:08:22.225475Z","iopub.status.idle":"2023-05-07T19:08:22.234655Z","shell.execute_reply.started":"2023-05-07T19:08:22.225445Z","shell.execute_reply":"2023-05-07T19:08:22.233767Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Set up\n\nset_seeds()\n\nDIR_PATH = \"/kaggle/input/facial-keypoints-detection/\"\ntraining_data = pd.read_csv(f\"{DIR_PATH}training.zip\")\ntest_data = pd.read_csv(f\"{DIR_PATH}test.zip\")\nid_lookup_table = pd.read_csv(f\"{DIR_PATH}IdLookupTable.csv\")\n\n@dataclass  \nclass ModelParams:\n    BATCH_SIZE: int = 64\n    VALID_SIZE: float = 0.2\n    N_EPOCHS: int = 100\n    IMG_SIZE: int = 96\n    OUTPUT_SIZE: int = 30 \n    S_OUTPUT_SIZE: int = 8\n    L_OUTPUT_SIZE: int = 22\n    LEARNING_RATE: float = 0.01\n\ndagshub.init(\"facial_reg_model\", \"caddis90\", mlflow=True)\nmlflow.set_tracking_uri('https://dagshub.com/caddis90/facial_reg_model.mlflow')\nmlflow.set_experiment(experiment_name=\"cnn\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using {device}\")\n\nl_dataset_cols = [\n    'left_eye_inner_corner_x','left_eye_inner_corner_y', 'left_eye_outer_corner_x',\n    'left_eye_outer_corner_y', 'right_eye_inner_corner_x','right_eye_inner_corner_y',\n    'right_eye_outer_corner_x','right_eye_outer_corner_y', 'left_eyebrow_inner_end_x',\n    'left_eyebrow_inner_end_y', 'left_eyebrow_outer_end_x','left_eyebrow_outer_end_y',\n    'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y', 'right_eyebrow_outer_end_x',\n    'right_eyebrow_outer_end_y', 'mouth_left_corner_x', 'mouth_left_corner_y',\n    'mouth_right_corner_x', 'mouth_right_corner_y', 'mouth_center_top_lip_x',\n    'mouth_center_top_lip_y', 'Image']\n\ns_dataset_cols = [\n    'left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x',\n    'right_eye_center_y','nose_tip_x', 'nose_tip_y',\n    'mouth_center_bottom_lip_x','mouth_center_bottom_lip_y','Image']\n\nl_dataset = training_data[l_dataset_cols].dropna()\ns_dataset = training_data[s_dataset_cols].dropna()","metadata":{"execution":{"iopub.status.busy":"2023-05-07T19:07:42.082465Z","iopub.execute_input":"2023-05-07T19:07:42.082829Z","iopub.status.idle":"2023-05-07T19:07:46.108037Z","shell.execute_reply.started":"2023-05-07T19:07:42.082778Z","shell.execute_reply":"2023-05-07T19:07:46.105975Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Repository initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Using cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data \n\nclass FacialKeypointsDataset(Dataset):\n    \n    def __init__(self, dataset, train=True, transform=None):\n        self.dataset = dataset\n        self.train = train\n        self.transform = transform\n\n    def get_image(self, idx):\n        image = np.fromstring(self.dataset.iloc[idx, -1], sep=' ', dtype = np.uint8)\n        image = image.astype(np.float32)\n        image = image.reshape(ModelParams.IMG_SIZE, ModelParams.IMG_SIZE, 1) \n        \n        return image\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):        \n        if self.train:\n            keypoints = self.dataset.iloc[idx, :-1].values.astype(np.float32)\n            total_keypoints = int(len(keypoints)/2)\n            keypoints = keypoints.reshape([total_keypoints, 2])\n        else:\n            keypoints = None\n        \n        if self.transform:\n            data_cols = self.dataset.columns.tolist()\n            sample = self.transform(image=self.get_image(idx), keypoints=keypoints, class_labels=data_cols[0:-1])\n            sample[\"keypoints\"] = torch.tensor(list(sum(sample[\"keypoints\"], ()))).float()\n        else:\n            sample = {\"image\": self.get_image(idx)}\n            \n        sample[\"image\"] = torch.from_numpy(sample[\"image\"].transpose(2, 0, 1)).float()\n        sample[\"image\"] = sample[\"image\"] / 255\n        \n        return sample\n        \n        \ndef prepare_dataloaders(dataset, valid_size, batch_size):\n    dataset_len = len(dataset)\n    dataset_indices = list(range(dataset_len))\n    np.random.shuffle(dataset_indices)\n    split = int(np.floor(valid_size * dataset_len))\n    train_idx, valid_idx = dataset_indices[split:], dataset_indices[:split]\n    \n    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_idx))\n    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(valid_idx))\n    \n    return train_loader, valid_loader\n\n\n# Utilities\n\ndef show_image(image, training_key_points, test_key_points=[]):  \n    plt.imshow(image, cmap=\"gray\")\n\n    training_key_points = training_key_points.reshape([15, 2])\n    plt.plot(training_key_points[:,0], training_key_points[:,1], 'gx')\n    \n    if len(test_key_points) > 0:        \n        test_key_points = test_key_points.reshape([15, 2])\n        plt.plot(test_key_points[:,0], test_key_points[:,1], 'rx')\n\n\ndef train(train_loader, valid_loader, model, optimizer, scheduler):\n    with mlflow.start_run():\n        mlflow.log_params(asdict(ModelParams()))\n        for epoch in progressbar.progressbar(range(ModelParams.N_EPOCHS)):\n            epoch_train_loss, epoch_valid_loss = 0.0, 0.0\n\n            model.train() \n            for i, batch in enumerate(train_loader):\n                optimizer.zero_grad()\n                output = model(batch['image'].to(device))\n                loss = criterion(output, batch['keypoints'].to(device))\n                loss.backward()\n                optimizer.step()\n                epoch_train_loss += loss.item()*batch['image'].size(0)\n                batch_train_rmse = rmse(output.cpu(), batch['keypoints'].cpu())\n\n            epoch_train_rmse = rmse.compute()\n            rmse.reset()\n\n            with torch.no_grad():\n                model.eval() \n                for i, batch in enumerate(valid_loader):\n                    output = model(batch['image'].to(device))\n                    loss = criterion(output, batch['keypoints'].to(device))\n                    epoch_valid_loss += loss.item()*batch['image'].size(0)\n                    batch_valid_rmse = rmse(output.cpu(), batch['keypoints'].cpu())\n\n                epoch_valid_rmse = rmse.compute()\n                rmse.reset()\n                epoch_train_loss = np.sqrt(epoch_train_loss/len(train_loader.sampler.indices))\n                epoch_valid_loss = np.sqrt(epoch_valid_loss/len(valid_loader.sampler.indices))\n                \n                scheduler.step(metrics=epoch_valid_loss)\n\n                mlflow.log_metric(\"train_loss\", epoch_train_loss, step=epoch)\n                mlflow.log_metric(\"valid_loss\", epoch_valid_loss, step=epoch)\n                mlflow.log_metric(\"train_rmse\", epoch_train_rmse, step=epoch)\n                mlflow.log_metric(\"valid_rmse\", epoch_valid_rmse, step=epoch)\n            \ndef predict(model, test_loader):    \n    model.eval()\n    with torch.no_grad():\n        for i, batch in progressbar.progressbar(enumerate(test_loader)):\n            output = model(batch['image'].to(device)).cpu().numpy()\n            output = np.clip(output, a_min=0, a_max=ModelParams.IMG_SIZE)\n            if i == 0:\n                test_predictions = output\n            else:\n                test_predictions = np.vstack((test_predictions, output))\n        \n    return test_predictions\n\ndef create_submission(predictions, prediction_features, id_lookup_table=id_lookup_table):\n    features = list(id_lookup_table['FeatureName'])\n    img_ids = list(id_lookup_table['ImageId']-1) \n\n    prediction_indices = [prediction_features.index(feature) for feature in features]\n\n    submission = pd.DataFrame({\n        \"RowId\": list(id_lookup_table['RowId']),\n        \"Location\": [predictions[x][y] for x, y in zip(img_ids, prediction_indices)]\n    })\n    submission.to_csv(\"submission.csv\",index = False)\n    print(\"Submission successful!\")\n       \n# Model\n\nl_resnet50 = models.resnet50(num_classes = ModelParams.L_OUTPUT_SIZE)\nl_resnet50.inplanes = ModelParams.IMG_SIZE\nl_resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\ns_resnet50 = models.resnet50(num_classes = ModelParams.S_OUTPUT_SIZE)\ns_resnet50.inplanes = ModelParams.IMG_SIZE\ns_resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\ntransformations = A.Compose([\n    A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.8),\n    A.Affine(shear=15, scale=1.0, p=0.2),\n    A.RandomBrightnessContrast(contrast_limit=0.5, brightness_limit=0.5, p=0.2),\n    A.OneOf([\n            A.GaussNoise(p=0.8),\n            A.RandomGamma(p=0.8),\n            A.Blur(p=0.8),\n        ], p=1.0),     \n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.2, border_mode=cv2.BORDER_CONSTANT),\n    ],\n    keypoint_params = A.KeypointParams(format = 'xy', remove_invisible=False)\n)\n\ns_trainset = FacialKeypointsDataset(s_dataset, transform=transformations)\nl_trainset = FacialKeypointsDataset(l_dataset, transform=transformations)\ntestset = FacialKeypointsDataset(test_data, train=False)\n\ns_train_loader, s_valid_loader = prepare_dataloaders(\n        s_trainset, \n        valid_size=ModelParams.VALID_SIZE, \n        batch_size=ModelParams.BATCH_SIZE\n)\nl_train_loader, l_valid_loader = prepare_dataloaders(\n        l_trainset, \n        valid_size=ModelParams.VALID_SIZE, \n        batch_size=ModelParams.BATCH_SIZE\n)\ntest_loader = DataLoader(testset, batch_size=ModelParams.BATCH_SIZE)\n\nl_model = l_resnet50\nl_model = l_model.to(device)\ns_model = s_resnet50\ns_model = s_model.to(device)\n\ncriterion = nn.MSELoss().to(device)\nrmse = MeanSquaredError(squared=False).to(device)\n\nl_optimizer = optim.Adam(l_model.parameters(), lr=ModelParams.LEARNING_RATE)\nl_scheduler = ReduceLROnPlateau(\n    optimizer=l_optimizer, \n    mode=\"min\", \n    factor=0.5,\n    patience=5,\n    min_lr=1e-15\n)\n\ns_optimizer = optim.Adam(s_model.parameters(), lr=ModelParams.LEARNING_RATE)\ns_scheduler = ReduceLROnPlateau(\n    optimizer=s_optimizer, \n    mode=\"min\", \n    factor=0.5,\n    patience=5,\n    min_lr=1e-15\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T19:11:07.793018Z","iopub.execute_input":"2023-05-07T19:11:07.793390Z","iopub.status.idle":"2023-05-07T19:11:08.731564Z","shell.execute_reply.started":"2023-05-07T19:11:07.793360Z","shell.execute_reply":"2023-05-07T19:11:08.730667Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train(\n    train_loader=l_train_loader, \n    valid_loader=l_valid_loader, \n    model=l_model, \n    optimizer=l_optimizer, \n    scheduler=l_scheduler\n)\n\ntrain(\n    train_loader=s_train_loader, \n    valid_loader=s_valid_loader, \n    model=s_model, \n    optimizer=s_optimizer, \n    scheduler=s_scheduler\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T19:11:13.408904Z","iopub.execute_input":"2023-05-07T19:11:13.409241Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  2% (2 of 100) |                        | Elapsed Time: 0:00:14 ETA:   0:11:04","output_type":"stream"}]},{"cell_type":"code","source":"l_predictions = predict(model=l_model, test_loader=test_loader)\ns_predictions = predict(model=s_model, test_loader=test_loader)\npredictions = np.hstack((l_predictions, s_predictions))\nprediction_features = l_dataset_cols[:-1] + s_dataset_cols[:-1]\ncreate_submission(predictions=predictions, prediction_features=prediction_features)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:05:38.082937Z","iopub.execute_input":"2023-05-06T19:05:38.083292Z","iopub.status.idle":"2023-05-06T19:05:44.216586Z","shell.execute_reply.started":"2023-05-06T19:05:38.083263Z","shell.execute_reply":"2023-05-06T19:05:44.215479Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"| |                             #                    | 13 Elapsed Time: 0:00:02\n| |                           #                      | 13 Elapsed Time: 0:00:02\n","output_type":"stream"},{"name":"stdout","text":"Submission successful!\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    test_key_points = model(plot_sample['image'].unsqueeze(0).to(device)).cpu().numpy()","metadata":{},"execution_count":null,"outputs":[]}]}