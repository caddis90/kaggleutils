{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installs \n\n!pip install mlflow dagshub progressbar2","metadata":{"execution":{"iopub.status.busy":"2023-04-27T19:17:19.799680Z","iopub.execute_input":"2023-04-27T19:17:19.800155Z","iopub.status.idle":"2023-04-27T19:17:19.805537Z","shell.execute_reply.started":"2023-04-27T19:17:19.800106Z","shell.execute_reply":"2023-04-27T19:17:19.804512Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Imports\n\nfrom dataclasses import dataclass, asdict\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torchvision import transforms, utils\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch import flatten\nfrom torchmetrics import MeanSquaredError\nimport progressbar\n\nimport dagshub\nimport mlflow\n\n\n# CSV files\n\nDIR_PATH = \"/kaggle/input/facial-keypoints-detection/\"\ntraining_data = pd.read_csv(f\"{DIR_PATH}training.zip\")\ntraining_data = training_data.dropna()\ntest_data = pd.read_csv(f\"{DIR_PATH}test.zip\")\nid_lookup_table = pd.read_csv(f\"{DIR_PATH}IdLookupTable.csv\")\n\n# Params\n\n@dataclass\nclass ModelParams:\n    BATCH_SIZE: int = 128\n    VALID_SIZE: float = 0.2\n    N_EPOCHS: int = 15\n    IMG_SIZE: int = 96\n    OUTPUT_SIZE: int = 30\n    LEARNING_RATE: float = 0.03\n    \ndagshub.init(\"facial_reg_model\", \"caddis90\", mlflow=True)\n\nmlflow.set_tracking_uri('https://dagshub.com/caddis90/facial_reg_model.mlflow')\nmlflow.set_experiment(experiment_name=\"cnn\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T19:39:06.344346Z","iopub.execute_input":"2023-04-27T19:39:06.344755Z","iopub.status.idle":"2023-04-27T19:39:11.429406Z","shell.execute_reply.started":"2023-04-27T19:39:06.344721Z","shell.execute_reply":"2023-04-27T19:39:11.428234Z"},"trusted":true},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"Repository initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n</pre>\n"},"metadata":{}},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='mlflow-artifacts:/626852b5f0d64f76a22d0244445f3383', creation_time=1682621968764, experiment_id='0', last_update_time=1682621968764, lifecycle_stage='active', name='cnn', tags={}>"},"metadata":{}}]},{"cell_type":"code","source":"# Data \n\nclass FacialKeypointsDataset(Dataset):\n    \n    def __init__(self, dataset, train=True, transform=None):\n        self.dataset = dataset.dropna()\n        self.train = train\n        self.transform = transform\n\n    def get_image(self, idx):\n        image = np.fromstring(self.dataset.iloc[idx, -1], sep=' ')\n        image = image.astype(np.float32)\n        image = image.reshape(ModelParams.IMG_SIZE, ModelParams.IMG_SIZE, 1) \n        return image\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):        \n        if self.train:\n            keypoints = self.dataset.iloc[idx, :-1].values.astype(np.float32)\n        else:\n            keypoints = None\n\n        sample = {'image': self.get_image(idx), 'keypoints': keypoints}\n        \n        if self.transform:\n            sample = self.transform(sample)\n            \n        return sample\n\n\ndef prepare_dataloaders(dataset, valid_size, batch_size):\n    dataset_len = len(dataset)\n    dataset_indices = list(range(dataset_len))\n    np.random.shuffle(dataset_indices)\n    split = int(np.floor(valid_size * dataset_len))\n    train_idx, valid_idx = dataset_indices[split:], dataset_indices[:split]\n    \n    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_idx))\n    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(valid_idx))\n    \n    return train_loader, valid_loader\n\n# Transforms\n\nclass Normalize(object):    \n    def __call__(self, sample):\n        image, keypoints = sample['image'], sample['keypoints']\n        \n        return {'image': image / 255., 'keypoints': keypoints}\n        \nclass ToTensor(object):\n    def __call__(self, sample):\n        image, keypoints = sample['image'], sample['keypoints']\n\n        # NumPy image: H x W x C versus Torch image: C X H X W\n        image = image.reshape(1, ModelParams.IMG_SIZE, ModelParams.IMG_SIZE)\n        \n        if keypoints is not None:\n            return {'image': torch.from_numpy(image), 'keypoints': torch.from_numpy(keypoints)}\n        else:\n            return {'image': torch.from_numpy(image)}\n\n# Utilities\n\ndef create_submission(predictions, id_lookup_table=id_lookup_table):\n    features_n = list(id_lookup_table['FeatureName'])\n    img_ids = list(id_lookup_table['ImageId']-1) \n\n    features_lst = [features_n.index(feature) for feature in features_n]\n\n    submission = pd.DataFrame({\n        \"RowId\": list(id_lookup_table['RowId']),\n        \"Location\": [predictions[x][y] for x, y in zip(img_ids, features_lst)]\n    })\n    submission.to_csv(\"submission.csv\",index = False)\n    \n\ndef show_image(image, training_key_points, test_key_points= None):\n    plt.imshow(image, cmap=\"gray\")\n\n    training_key_points = training_key_points.reshape([15, 2])\n    plt.plot(training_key_points[:,0], training_key_points[:,1], 'gx')\n    \n    if len(test_key_points) > 0:\n        test_key_points = test_key_points.reshape([15, 2])\n        plt.plot(test_key_points[:,0], test_key_points[:,1], 'rx')\n        \n    \n# Models \n\nclass CNN(nn.Module):\n    def __init__(self, output_size):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(in_features=64*12*12, out_features=1024)\n        self.fc2 = nn.Linear(in_features=1024, out_features=output_size)\n        self.dropout = nn.Dropout(p=0.3)\n        \n    def forward(self, x):\n        \"\"\"\n        2D Convolution and Pooling\n        ---------------------------\n        (N, C_IN, H_IN, W_IN) -> (N, C_OUT, H_OUT, W_OUT)\n        H_OUT = (H_IN + 2 * PADDING - DILATION - (KERNEL_SZ-1) - 1) / STRIDE + 1\n        W_OUT = (W_IN + 2 * PADDING - DILATION - (KERNEL_SZ-1) - 1) / STRIDE + 1\n        \n        Fully Connected \n        ---------------\n        (H_IN, W_IN) * (H_FC, W_FC) = (H_IN, W_FC)\n        \"\"\"\n        \n        x = self.conv1(x) \n        x = F.relu(x)\n        # torch.Size([128, 1, 96, 96]) -> torch.Size([128, 16, 48, 48])\n        x = self.pool(x)\n        \n        x = self.conv2(x)\n        x = F.relu(x)\n        # torch.Size([128, 16, 48, 48]) -> torch.Size([128, 32, 24, 24])\n        x = self.pool(x)\n        \n        x = self.conv3(x)\n        x = F.relu(x)\n        # torch.Size([128, 32, 24, 24]) -> torch.Size([128, 64, 12, 12])\n        x = self.pool(x)\n        \n        # torch.Size([128, 64, 12, 12]) -> torch.Size([128, 9216])\n        x = x.view(-1, 64*12*12)\n        x = self.dropout(x)\n        \n        # torch.Size([128, 9216]) -> torch.Size([128, 1024])\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n\n        # torch.Size([128, 1024]) -> torch.Size([128, 30])\n        x = self.fc2(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-27T19:18:22.616369Z","iopub.execute_input":"2023-04-27T19:18:22.616790Z","iopub.status.idle":"2023-04-27T19:18:22.647786Z","shell.execute_reply.started":"2023-04-27T19:18:22.616755Z","shell.execute_reply":"2023-04-27T19:18:22.646470Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"transformations = transforms.Compose([Normalize(), ToTensor()])\n\ntrainset = FacialKeypointsDataset(training_data, transform=transformations)\ntestset = FacialKeypointsDataset(test_data, train=False, transform=transformations)\n\ntrain_loader, valid_loader = prepare_dataloaders(\n        trainset, \n        valid_size=ModelParams.VALID_SIZE, \n        batch_size=ModelParams.BATCH_SIZE\n)\ntest_loader = DataLoader(testset, batch_size=ModelParams.BATCH_SIZE)\n\nmodel = CNN(output_size=ModelParams.OUTPUT_SIZE)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\ncriterion = nn.MSELoss()\nrmse = MeanSquaredError(squared=False)\noptimizer = optim.Adam(model.parameters(), lr=ModelParams.LEARNING_RATE)\nscheduler = ExponentialLR(optimizer, gamma=0.9)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T19:41:03.014533Z","iopub.execute_input":"2023-04-27T19:41:03.015366Z","iopub.status.idle":"2023-04-27T19:41:03.139874Z","shell.execute_reply.started":"2023-04-27T19:41:03.015315Z","shell.execute_reply":"2023-04-27T19:41:03.138635Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"with mlflow.start_run():\n    mlflow.log_params(asdict(ModelParams()))\n    for epoch in progressbar.progressbar(range(ModelParams.N_EPOCHS)):\n        train_loss, valid_loss, train_rmse, valid_rmse = 0.0, 0.0, 0.0, 0.0\n\n        model.train() \n        for batch in train_loader:\n            optimizer.zero_grad()\n            output = model(batch['image'].to(device))\n            loss = criterion(output, batch['keypoints'].to(device))\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()*batch['image'].size(0)\n            train_rmse += rmse(output, batch['keypoints'])\n            \n        scheduler.step()\n\n        model.eval() \n        for batch in valid_loader:\n            output = model(batch['image'].to(device))\n            loss = criterion(output, batch['keypoints'].to(device))\n            valid_loss += loss.item()*batch['image'].size(0)\n            valid_rmse += rmse(output, batch['keypoints'])\n\n        train_loss = np.sqrt(train_loss/len(train_loader.sampler.indices))\n        valid_loss = np.sqrt(valid_loss/len(valid_loader.sampler.indices))\n        train_rmse = train_rmse/len(valid_loader.sampler.indices)\n        valid_rmse = valid_rmse/len(valid_loader.sampler.indices)\n        \n        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n        mlflow.log_metric(\"valid_loss\", valid_loss, step=epoch)\n        mlflow.log_metric(\"train_rmse\", train_rmse, step=epoch)\n        mlflow.log_metric(\"valid_rmse\", valid_rmse, step=epoch)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T19:54:25.554028Z","iopub.execute_input":"2023-04-27T19:54:25.554517Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  6% (1 of 15) |#                        | Elapsed Time: 0:00:16 ETA:   0:03:53","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\n\nwith torch.no_grad():\n    for i, batch in enumerate(test_loader):\n        output = model(batch['image'].to(device)).cpu().numpy()\n        output = np.clip(output, a_min=0, a_max=96)\n        if i == 0:\n            test_predictions = output\n        else:\n            test_predictions = np.vstack((test_predictions, output))\n    \n\ncreate_submission(test_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T19:36:23.483206Z","iopub.execute_input":"2023-04-27T19:36:23.483621Z","iopub.status.idle":"2023-04-27T19:36:29.555557Z","shell.execute_reply.started":"2023-04-27T19:36:23.483588Z","shell.execute_reply":"2023-04-27T19:36:29.554172Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"model.eval()\nplot_sample = trainset[0]\n  \nwith torch.no_grad():\n    test_key_points = model(plot_sample['image'].to(device)).cpu().numpy()\nimage = plot_sample[\"image\"].numpy()\nimage = image.reshape(ModelParams.IMG_SIZE, ModelParams.IMG_SIZE, 1)\n    \nshow_image(image=image, training_key_points=plot_sample[\"keypoints\"].numpy(), test_key_points=test_key_points)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:29:10.036187Z","iopub.execute_input":"2023-04-26T18:29:10.036606Z","iopub.status.idle":"2023-04-26T18:29:10.316791Z","shell.execute_reply.started":"2023-04-26T18:29:10.036571Z","shell.execute_reply":"2023-04-26T18:29:10.315404Z"},"trusted":true},"execution_count":null,"outputs":[]}]}