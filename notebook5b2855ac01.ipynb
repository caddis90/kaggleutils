{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installs \n\n# !pip install mlflow dagshub progressbar2 GPUtil albumentations","metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:14:08.121577Z","iopub.execute_input":"2023-05-01T18:14:08.122363Z","iopub.status.idle":"2023-05-01T18:14:08.126958Z","shell.execute_reply.started":"2023-05-01T18:14:08.122316Z","shell.execute_reply":"2023-05-01T18:14:08.125862Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Imports\n\nfrom dataclasses import dataclass, asdict\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torchvision import models, transforms, utils\nfrom torch import nn, optim\nfrom PIL import Image\nimport torch.nn.functional as F\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch import flatten\nfrom torchmetrics import MeanSquaredError\nimport progressbar\nimport dagshub\nimport mlflow\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# CSV files\n\nDIR_PATH = \"/kaggle/input/facial-keypoints-detection/\"\ntraining_data = pd.read_csv(f\"{DIR_PATH}training.zip\")\ntest_data = pd.read_csv(f\"{DIR_PATH}test.zip\")\nid_lookup_table = pd.read_csv(f\"{DIR_PATH}IdLookupTable.csv\")\n\n# Params\n\n@dataclass\nclass ModelParams:\n    BATCH_SIZE: int = 128\n    VALID_SIZE: float = 0.2\n    N_EPOCHS: int = 15\n    IMG_SIZE: int = 96\n    OUTPUT_SIZE: int = 30 \n    S_OUTPUT_SIZE: int = 8\n    L_OUTPUT_SIZE: int = 22\n    LEARNING_RATE: float = 0.03  \n\ntorch.manual_seed(0)\nnp.random.seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\ntorch.backends.cudnn.deterministic=True\n\ndagshub.init(\"facial_reg_model\", \"caddis90\", mlflow=True)\n\nmlflow.set_tracking_uri('https://dagshub.com/caddis90/facial_reg_model.mlflow')\nmlflow.set_experiment(experiment_name=\"cnn\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using {device}\")\n\n# training_data.isna().sum()\n\ns_dataset_cols = [\n    'left_eye_inner_corner_x','left_eye_inner_corner_y', 'left_eye_outer_corner_x',\n    'left_eye_outer_corner_y', 'right_eye_inner_corner_x','right_eye_inner_corner_y',\n    'right_eye_outer_corner_x','right_eye_outer_corner_y', 'left_eyebrow_inner_end_x',\n    'left_eyebrow_inner_end_y', 'left_eyebrow_outer_end_x','left_eyebrow_outer_end_y',\n    'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y', 'right_eyebrow_outer_end_x',\n    'right_eyebrow_outer_end_y', 'mouth_left_corner_x', 'mouth_left_corner_y',\n    'mouth_right_corner_x', 'mouth_right_corner_y', 'mouth_center_top_lip_x',\n    'mouth_center_top_lip_y', 'Image']\n\nl_dataset_cols = [\n    'left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x',\n    'right_eye_center_y','nose_tip_x', 'nose_tip_y',\n    'mouth_center_bottom_lip_x','mouth_center_bottom_lip_y','Image']\n\nl_dataset = training_data[l_dataset_cols].dropna()\ns_dataset = training_data[s_dataset_cols].dropna()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:26:29.975278Z","iopub.execute_input":"2023-05-01T20:26:29.975950Z","iopub.status.idle":"2023-05-01T20:26:33.966304Z","shell.execute_reply.started":"2023-05-01T20:26:29.975908Z","shell.execute_reply":"2023-05-01T20:26:33.965099Z"},"trusted":true},"execution_count":198,"outputs":[{"output_type":"display_data","data":{"text/plain":"Repository initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Using cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# GPU \n\ndef free_gpu_cache():\n    print(\"Initial GPU usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU usage after emptying the cache\")\n    gpu_usage()\n    \n\n# Data \n\nclass FacialKeypointsDataset(Dataset):\n    \n    def __init__(self, dataset, train=True, transform=None):\n        self.dataset = dataset\n        self.train = train\n        self.transform = transform\n\n    def get_image(self, idx):\n        image = np.fromstring(self.dataset.iloc[idx, -1], sep=' ')\n        image = image.astype(np.float32)\n        image = image.reshape(ModelParams.IMG_SIZE, ModelParams.IMG_SIZE, 1) \n        return image\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):        \n        if self.train:\n            keypoints = self.dataset.iloc[idx, :-1].values.astype(np.float32)\n            total_keypoints = int(len(keypoints)/2)\n            keypoints = keypoints.reshape([total_keypoints, 2])\n        else:\n            keypoints = None\n        \n        if self.transform:\n            data_cols = self.dataset.columns.tolist()\n            sample = self.transform(image=self.get_image(idx), keypoints=keypoints, class_labels=data_cols[0:-1])\n            sample[\"keypoints\"] = torch.tensor(list(sum(sample[\"keypoints\"], ()))).float()\n        else:\n            sample = {\"image\": self.get_image(idx)}\n            \n        sample[\"image\"] = torch.from_numpy(sample[\"image\"].transpose(2, 0, 1)).float()\n        \n        return sample\n        \n        \ndef prepare_dataloaders(dataset, valid_size, batch_size):\n    dataset_len = len(dataset)\n    dataset_indices = list(range(dataset_len))\n    np.random.shuffle(dataset_indices)\n    split = int(np.floor(valid_size * dataset_len))\n    train_idx, valid_idx = dataset_indices[split:], dataset_indices[:split]\n    \n    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_idx))\n    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(valid_idx))\n    \n    return train_loader, valid_loader\n\n\n# Utilities\n\ndef create_submission(predictions, id_lookup_table=id_lookup_table):\n    features_n = list(id_lookup_table['FeatureName'])\n    img_ids = list(id_lookup_table['ImageId']-1) \n\n    features_lst = [features_n.index(feature) for feature in features_n]\n\n    submission = pd.DataFrame({\n        \"RowId\": list(id_lookup_table['RowId']),\n        \"Location\": [predictions[x][y] for x, y in zip(img_ids, features_lst)]\n    })\n    submission.to_csv(\"submission.csv\",index = False)\n    \n\ndef show_image(image, training_key_points, test_key_points=[]):  \n    plt.imshow(image, cmap=\"gray\")\n\n    training_key_points = training_key_points.reshape([15, 2])\n    plt.plot(training_key_points[:,0], training_key_points[:,1], 'gx')\n    \n    if len(test_key_points) > 0:        \n        test_key_points = test_key_points.reshape([15, 2])\n        plt.plot(test_key_points[:,0], test_key_points[:,1], 'rx')\n\n\ndef train(train_loader, valid_loader, model, optimizer, scheduler):\n    with mlflow.start_run():\n        mlflow.log_params(asdict(ModelParams()))\n        for epoch in progressbar.progressbar(range(ModelParams.N_EPOCHS)):\n            epoch_train_loss, epoch_valid_loss = 0.0, 0.0\n\n            model.train() \n            for i, batch in enumerate(train_loader):\n                optimizer.zero_grad()\n                output = model(batch['image'].to(device))\n                loss = criterion(output, batch['keypoints'].to(device))\n                loss.backward()\n                optimizer.step()\n                epoch_train_loss += loss.item()*batch['image'].size(0)\n                batch_train_rmse = rmse(output.cpu(), batch['keypoints'].cpu())\n\n            epoch_train_rmse = rmse.compute()\n            rmse.reset()\n            scheduler.step()\n\n            with torch.no_grad():\n                model.eval() \n                for i, batch in enumerate(valid_loader):\n                    output = model(batch['image'].to(device))\n                    loss = criterion(output, batch['keypoints'].to(device))\n                    epoch_valid_loss += loss.item()*batch['image'].size(0)\n                    batch_valid_rmse = rmse(output.cpu(), batch['keypoints'].cpu())\n\n                epoch_valid_rmse = rmse.compute()\n                rmse.reset()\n\n                epoch_train_loss = np.sqrt(epoch_train_loss/len(train_loader.sampler.indices))\n                epoch_valid_loss = np.sqrt(epoch_valid_loss/len(valid_loader.sampler.indices))\n\n                mlflow.log_metric(\"train_loss\", epoch_train_loss, step=epoch)\n                mlflow.log_metric(\"valid_loss\", epoch_valid_loss, step=epoch)\n                mlflow.log_metric(\"train_rmse\", epoch_train_rmse, step=epoch)\n                mlflow.log_metric(\"valid_rmse\", epoch_valid_rmse, step=epoch)\n            \n# Model\n\nl_resnet50 = models.resnet50(num_classes = ModelParams.L_OUTPUT_SIZE)\nl_resnet50.inplanes = ModelParams.IMG_SIZE\nl_resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\ns_resnet50 = models.resnet50(num_classes = ModelParams.S_OUTPUT_SIZE)\ns_resnet50.inplanes = ModelParams.IMG_SIZE\ns_resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\ntransformations = A.Compose([\n    A.RandomBrightnessContrast(p=0.2)\n    ],\n    keypoint_params = A.KeypointParams(format = 'xy', remove_invisible=False)\n)\n\ns_trainset = FacialKeypointsDataset(s_dataset, transform=transformations)\nl_trainset = FacialKeypointsDataset(l_dataset, transform=transformations)\ntestset = FacialKeypointsDataset(test_data, train=False)\n\ns_train_loader, s_valid_loader = prepare_dataloaders(\n        s_trainset, \n        valid_size=ModelParams.VALID_SIZE, \n        batch_size=ModelParams.BATCH_SIZE\n)\nl_train_loader, l_valid_loader = prepare_dataloaders(\n        l_trainset, \n        valid_size=ModelParams.VALID_SIZE, \n        batch_size=ModelParams.BATCH_SIZE\n)\ntest_loader = DataLoader(testset, batch_size=ModelParams.BATCH_SIZE)\n\nl_model = l_resnet50\nl_model = l_model.to(device)\ns_model = s_resnet50\ns_model = s_model.to(device)\n\ncriterion = nn.MSELoss().to(device)\nrmse = MeanSquaredError(squared=False).to(device)\n\nl_optimizer = optim.Adam(l_model.parameters(), lr=ModelParams.LEARNING_RATE)\nl_scheduler = ExponentialLR(l_optimizer, gamma=0.9)\n\ns_optimizer = optim.Adam(s_model.parameters(), lr=ModelParams.LEARNING_RATE)\ns_scheduler = ExponentialLR(s_optimizer, gamma=0.9)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:33:10.097001Z","iopub.execute_input":"2023-05-01T20:33:10.097379Z","iopub.status.idle":"2023-05-01T20:33:10.862178Z","shell.execute_reply.started":"2023-05-01T20:33:10.097343Z","shell.execute_reply":"2023-05-01T20:33:10.860850Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"train(\n    train_loader=l_train_loader, \n    valid_loader=l_valid_loader, \n    model=l_model, \n    optimizer=l_optimizer, \n    scheduler=l_scheduler\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:33:43.968709Z","iopub.execute_input":"2023-05-01T20:33:43.969943Z","iopub.status.idle":"2023-05-01T20:33:44.860244Z","shell.execute_reply.started":"2023-05-01T20:33:43.969887Z","shell.execute_reply":"2023-05-01T20:33:44.858682Z"},"trusted":true},"execution_count":206,"outputs":[{"name":"stderr","text":"  0% (0 of 15) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/118023204.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n","\u001b[0;32m/tmp/ipykernel_23/2064210212.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, model, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keypoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3289\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3291\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3292\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (22) must match the size of tensor b (8) at non-singleton dimension 1"],"ename":"RuntimeError","evalue":"The size of tensor a (22) must match the size of tensor b (8) at non-singleton dimension 1","output_type":"error"}]},{"cell_type":"code","source":"model.eval()\n\nwith torch.no_grad():\n    for i, batch in progressbar.progressbar(enumerate(test_loader)):\n        output = model(batch['image'].to(device)).cpu().numpy()\n        output = np.clip(output, a_min=0, a_max=96)\n        if i == 0:\n            test_predictions = output\n        else:\n            test_predictions = np.vstack((test_predictions, output))\n    \ncreate_submission(test_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:01:37.688426Z","iopub.execute_input":"2023-05-01T20:01:37.689076Z","iopub.status.idle":"2023-05-01T20:01:40.337592Z","shell.execute_reply.started":"2023-05-01T20:01:37.689034Z","shell.execute_reply":"2023-05-01T20:01:40.336544Z"},"trusted":true},"execution_count":172,"outputs":[{"name":"stderr","text":"| |                        #                         | 13 Elapsed Time: 0:00:02\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    test_key_points = model(plot_sample['image'].unsqueeze(0).to(device)).cpu().numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_sample = l_trainset[0]\n# image = plot_sample[\"image\"].numpy()\n# image_height = image.shape[1]\n# image_width = image.shape[2]\n# image = image.reshape(image_height, image_width, 1)   \n# show_image(image=image, training_key_points=plot_sample[\"keypoints\"].numpy())#, test_key_points=test_key_points)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:26:33.968624Z","iopub.execute_input":"2023-05-01T20:26:33.969001Z","iopub.status.idle":"2023-05-01T20:26:33.973737Z","shell.execute_reply.started":"2023-05-01T20:26:33.968960Z","shell.execute_reply":"2023-05-01T20:26:33.972495Z"},"trusted":true},"execution_count":199,"outputs":[]}]}